{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Langchain以其模块化和集成性胜出，用开发LLM的通用框架。与此相比，LlamaIndex是为LLMs学习私有知识，擅长搜索和检索，专门用于构建RAG系统的框架。\n",
    "https://docs.llamaindex.ai/en/stable/index.html\n",
    "\"\"\"\n",
    "import os\n",
    "from wayne_utils import load_data, save_data\n",
    "import jieba\n",
    "\n",
    "# 基础包\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.schema import TextNode, Document\n",
    "\n",
    "embed_model = None\n",
    "llm = None\n",
    "\n",
    "doc_dir = \"/home/jiangpeiwen2/jiangpeiwen2/projects/text2table_preprocess/CPL/raw/FirstCollection/doc\"\n",
    "data_path = \"/home/jiangpeiwen2/jiangpeiwen2/projects/text2table_preprocess/CPL/pairs/texts.text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据加载为Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"批量数据加载\n",
    "1. SimpleDirectoryReader: 批量加载数据为Document列表，指定目录和编码格式，可迭代子目录，指定后缀或禁止后缀，\n",
    "    - 支持文件格式：无后缀（默认为txt）, txt, csv, docx, epub, ipynb, jpe, md, mp3,pdf, ppt等\n",
    "    - 不支持格式：JSON，建议使用专门的json加载器\n",
    "2. wayne_utils.load_data: 从文件加载数据，支持json, txt, csv, excel等格式。需要后续手动处理为Document列表\n",
    "\"\"\"\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir = doc_dir,\n",
    "    # input_files = [],                 # 指定文件列表\n",
    "    # required_exts=[\".txt\"],           # 指定的后缀\n",
    "    # exclude = [\".txt\"],               # 排除的后缀\n",
    "    # recursive = True,                 # 是否递归子目录\n",
    "    # filename_as_id = True,            # 是否使用文件名作为Document的id\n",
    "    # encoding = \"utf-8\"                  # \"utf-16\"\n",
    ")\n",
    "documents = reader.load_data()\n",
    "# 或者如下\n",
    "texts = load_data(data_path, \"text\")\n",
    "documents = []\n",
    "for text in texts[:20]:\n",
    "    title = text.split(\"###\")[0]\n",
    "    content = text.split(\"###\")[1]\n",
    "    documents.append( Document(text=content, metadata={\"title\": title}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Document切分为Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"chunk切分\n",
    "1. 手动构建chunk然后实例化为Node\n",
    "2. SentenceSplitter/SimpleNodeParser：分句器/节点解析器，直接从Document列表进行批量切分为Nodes\n",
    "\"\"\"\n",
    "\n",
    "# 自行进行切割Node化\n",
    "chunks = []\n",
    "node_list = [ TextNode( text=chunks[i], id_=str(i), metadata={} ) for i in range(len(chunks)) ]\n",
    "\n",
    "# 或者使用SentenceSplitter进行切分\n",
    "def chinese_tokenizer(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "sent_spliter = SentenceSplitter(\n",
    "    chunk_size = 128,\n",
    "    chunk_overlap = 16,\n",
    "    tokenizer = chinese_tokenizer,\n",
    "    paragraph_separator = \"\\\\n\",\n",
    "    secondary_chunking_regex = None     # 正则表达式，用于将段落拆分为句子\n",
    ")\n",
    "# 分割单个Document\n",
    "node = sent_spliter.split_text( documents[0].text )\n",
    "# 成批量处理Documents\n",
    "node_list = sent_spliter.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "\"\"\"\n",
    "此外，还可设置node之间的关系，形成图结构\n",
    "node1.relationships[DocumentRelationship.NEXT] = node2.get_doc_id()\n",
    "node2.relationships[DocumentRelationship.PREVIOUS] = node1.get_doc_id()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Embed和LLMs加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语言模型\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 1\n",
    "embedding_model_path = \"/home/jiangpeiwen2/jiangpeiwen2/projects/TKGT/Hybrid_RAG/retriever/embed_model/sentence-transformer\"\n",
    "llm_path = \"/home/jiangpeiwen2/jiangpeiwen2/workspace/LLMs/glm-4-9b-chat\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding( model_name = embedding_model_path )\n",
    "Settings.embed_model = embed_model\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name = llm_path,\n",
    "    tokenizer_name  = llm_path,\n",
    "    model_kwargs={\"trust_remote_code\":True},\n",
    "    tokenizer_kwargs={\"trust_remote_code\":True}\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Node list向量索引与持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Faiss向量库：稠密向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embeddings存储在index内部，不论是Faiss还是VectorStoreIndex。具体而言，如果传入的是Node list，则要求所有node的embeddings都是已存在。\n",
    "使用Faiss数据库保存向量，在查询期间，索引使用Faiss查询前k个嵌入，并返回相应的索引\"\"\"\n",
    "# ! pip install llama-index-vector-stores-faiss faiss-gpu faiss-cpu\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "# 对node list进行embedding，并将嵌入结果存储在Node数据结构中\n",
    "embed_model( node_list )\n",
    "d = len(node_list[0].embedding)\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_vector_store = FaissVectorStore(faiss_index=faiss_index)       # 或者从from_persist_path\n",
    "new_ids = faiss_vector_store.add(node_list)\n",
    "# vector_store.persist( \"persist/index\" )\n",
    "    \n",
    "# 直接存储Faiss数据库索引\n",
    "faiss_vector_store.persist( \"persist/faiss_store\" )\n",
    "faiss_vector_store = FaissVectorStore.from_persist_path( \"persist/faiss_store\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 VectorStoreIndex：稠密向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\"\"\"创建对node list中每个node/chunk的向量和索引结构，通常需要先使用StorageContext创建索引结构，然后使用Indexer创建向量\n",
    "StorageContext：存储上下文，里面保存docstore、index_store、vector_store、graph_store和property_graph_store\n",
    "\"\"\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    # docstore: Optional[BaseDocumentStore] = None,\n",
    "    # index_store: Optional[BaseIndexStore] = None,\n",
    "    # vector_store: Optional[BasePydanticVectorStore] = None,\n",
    "    # image_store: Optional[BasePydanticVectorStore] = None,\n",
    "    # vector_stores: Optional[Dict[str, BasePydanticVectorStore]] = None,\n",
    "    # graph_store: Optional[GraphStore] = None,\n",
    "    # property_graph_store: Optional[PropertyGraphStore] = None,\n",
    "    # persist_dir: Optional[str] = None,                                        # 持久化目录，如果传入这个参数，上述索引都从这个目录直接加载；否则要么使用传入的，要么直接实例化空类。\n",
    ")\n",
    "\n",
    "# 将文档数据（node）加入存储\n",
    "storage_context.docstore.add_documents( node_list )\n",
    "\n",
    "# 直接使用VectorStoreIndex实例化对node_list进行embedding后获得索引index，但不会保存vector\n",
    "# 也可以使用其from_vector_store方法，传入已有的向量存储（如Faiss），直接获得索引\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes = node_list,\n",
    "    # use_async: bool = False,\n",
    "    # store_nodes_override: bool = False,\n",
    "    # embed_model: Optional[EmbedType] = None,                  # 要么自己传入，要么使用Settings的embed_model，只能产生稠密的向量索引\n",
    "    # insert_batch_size: int = 2048,\n",
    "    # # parent class params\n",
    "    # objects: Optional[Sequence[IndexNode]] = None,\n",
    "    # index_struct: Optional[IndexDict] = None,\n",
    "    storage_context = storage_context,\n",
    "    # callback_manager: Optional[CallbackManager] = None,\n",
    "    # transformations: Optional[List[TransformComponent]] = None,\n",
    "    show_progress = True,\n",
    ")\n",
    "storage_context.vector_store.stores_text = True\n",
    "# 已经包含文档、向量、索引三部分的要素\n",
    "storage_context.persist( \"persist/normal_storage_context\" )\n",
    "# 从持久化存储中恢复索引并作为检索器使用\n",
    "storage_context = StorageContext.from_defaults( persist_dir = \"persist/normal_storage_context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Qdrant向量库：稀疏向量+稠密向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"稀疏向量主要是TF-IDF/BM25等，在Llama_index中只有特定向量库插件原生支持稀疏向量\n",
    "Qdrant的 BM42，需要使用docker container启动一个客户端client\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant\n",
    "```\n",
    "此外，该向量库不支持仅稀疏查询\n",
    "\"\"\"\n",
    "# ! pip install llama-index llama-index-vector-stores-qdrant fastembed\n",
    "from fastembed import SparseTextEmbedding\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "# （1）从fastembed加载稀疏模型\n",
    "sparse_model = SparseTextEmbedding(\n",
    "    model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")\n",
    "# 直接embedding\n",
    "# node_text_list = [ node_list[i].text for i in range(len(node_list)) ]\n",
    "# sparse_embeddings = sparse_model.embed( node_text_list )\n",
    "\n",
    "\n",
    "# （2）构建Qdrant向量数据库的客户端和存储实例，持久化有两种模式，一种是直接传入client，另一种是传入path。持久化通过collection_name获取。\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient( \"http://localhost:6333\" )      # \"http://localhost:6333\", path=\"persist/hybrid2_storage_context\"\n",
    "# aclient = qdrant_client.AsyncQdrantClient(path=\"persist/hybrid2_storage_context\")\n",
    "\n",
    "if client.collection_exists(\"llama2_bm42\"):\n",
    "    client.delete_collection(\"llama2_bm42\")\n",
    "\n",
    "sparse_vector_store = QdrantVectorStore(\n",
    "    collection_name=\"llama2_bm42\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")\n",
    "\n",
    "\n",
    "# （3）混合索引，storage_context在初始化时已经包含了稀疏向量，然后再加入稠密向量（必须）\n",
    "storage_context = StorageContext.from_defaults(vector_store=sparse_vector_store)\n",
    "\n",
    "hybrid_index = VectorStoreIndex(\n",
    "    nodes = node_list,\n",
    "    storage_context = storage_context,\n",
    "    # dense embedding model\n",
    "    embed_model=embed_model,\n",
    "    show_progress = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （4）RAG\n",
    "chat_engine = hybrid_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    llm=llm,\n",
    ")\n",
    "response = chat_engine.chat(\"对借款期限没有约定或者约定不明确应该怎么处理？\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Faiss向量数据库检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "faiss_vector_store = FaissVectorStore.from_persist_path( \"persist/faiss_store\" )\n",
    "query_string = \"双方口头约定月利率1.5%。从2013年5月份起胡军、李薇及其亲属每月通过转账向王红支付利息直至2014年3月27日。2015年胡军、李薇偿还了2万元本金\"\n",
    "query = VectorStoreQuery( \n",
    "    query_embedding = embed_model( [TextNode( text=query_string )] )[0].embedding, \n",
    "    similarity_top_k = 3,\n",
    "    query_str = query_string\n",
    ")\n",
    "query_results = faiss_vector_store.query( query )\n",
    "for _id in query_results.ids:\n",
    "    print(node_list[int(_id)-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 基于VectorStoreIndex的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults( persist_dir = \"persist/normal_storage_context\")\n",
    "vector_index = load_index_from_storage(storage_context)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=3, score = 0.1)\n",
    "query = \"\\\\n本院认为：本案争议焦点在于原告与被告李影之间的借贷关系是否成立并生效以及在此前提之下被告梁进是否负有还款义务。\"\n",
    "vector_retriever.retrieve( query )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 基于Qdrant向量库的混合检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "客户端自动管理持久化\n",
    "\"\"\"\n",
    "import qdrant_client\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"llama2_bm42\",\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")\n",
    "# 直接从现有的collection的vector_store中获取\n",
    "loaded_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chat_engine = loaded_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    llm=llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"对借款期限没有约定或者约定不明确应该怎么处理？\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 稀疏检索：BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "由于LLama_index原生不支持仅稀疏查询，可以使用rank_bm25等包实现\n",
    "!pip install rank_bm25\n",
    "!pip install llama-index-retrievers-bm25\n",
    "BM25Retriever是检索器，不存储数据，只是在数据上进行检索\n",
    "\"\"\"\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "topk = 3\n",
    "query = \"对借款期限没有约定或者约定不明确应该怎么处理？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_bm25_retriever =  BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=topk, verbose=True,\n",
    "    tokenizer=chinese_tokenizer,\n",
    "    language=\"zh\"\n",
    ")\n",
    "llama_bm25_retriever.retrieve( query )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "class Rank_BM25_Retriever:\n",
    "    \"\"\"\n",
    "    param node_list: llamainxde框架处理后的Node list\n",
    "    param similarity_top_k: 返回前几个\n",
    "    param score：threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, node_list, similarity_top_k = 2, score = 0.7):\n",
    "        self.topk = similarity_top_k\n",
    "        self.score = score\n",
    "        self.text_list = [ node.text for node in node_list]\n",
    "        corpus = [ chinese_tokenizer( sentence ) for sentence in self.text_list]\n",
    "        try:\n",
    "            self.retriever = BM25Okapi( corpus ) if len(corpus) > 0 else None\n",
    "        except:\n",
    "            raise Exception(corpus)\n",
    "        \n",
    "    \n",
    "    def from_nodes_to_list( self, nodes_list ):\n",
    "        ret_list = []\n",
    "        for node in nodes_list:\n",
    "            ret_list.append( node.text )\n",
    "        return ret_list\n",
    "    \n",
    "    def retrieve( self, query):\n",
    "        tokenized_query = chinese_tokenizer( query )\n",
    "        if self.retriever==None:\n",
    "            return []\n",
    "        doc_scores = self.retriever.get_scores( tokenized_query )\n",
    "        top_k_values, top_k_indices = self.find_top_k_elements(doc_scores, self.topk)\n",
    "\n",
    "        max_abs_val = np.max(np.abs(top_k_values))  \n",
    "        normalized_top_k_values = top_k_values / max_abs_val\n",
    "        ret = []\n",
    "        for i in range( len( normalized_top_k_values)):\n",
    "            sentence = self.text_list[ top_k_indices[i] ]\n",
    "            score = normalized_top_k_values[i]\n",
    "            if score>= self.score:\n",
    "                ret.append( {\"text\": sentence, \"score\": score})\n",
    "        return ret\n",
    "\n",
    "    def find_top_k_elements( self, array, topk):  \n",
    "        # 使用一个列表来存储元素及其索引的元组  \n",
    "        indexed_array = [(value, index) for index, value in enumerate(array)]  \n",
    "        \n",
    "        # 使用 heapq.nlargest 找到前 k 个最大的元素及其索引  \n",
    "        top_k_elements = heapq.nlargest(topk, indexed_array, key=lambda x: x[0])  \n",
    "        \n",
    "        # 提取值和索引  \n",
    "        top_k_values = [element[0] for element in top_k_elements]  \n",
    "        top_k_indices = [element[1] for element in top_k_elements]  \n",
    "        \n",
    "        return top_k_values, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapi_bm25_retriever = Rank_BM25_Retriever( node_list, similarity_top_k=3, score=0.1 )\n",
    "kapi_bm25_retriever.retrieve( query )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 进阶：自定义Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "from llama_index.core import KeywordTableIndex, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import KeywordTableSimpleRetriever, VectorIndexRetriever \n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "class Hybrid_Retriever:\n",
    "    \"多路召回Retrieval+重排ReRank\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        node_list:list, \n",
    "        hybrid_config:list[bool] = None, \n",
    "        embed_model_path:str = None, \n",
    "        llm_path:Optional[str] = None, \n",
    "        topk=3,\n",
    "        topk_total=5,\n",
    "        threshold=0.7, \n",
    "        gpu_id:Optional[int]=None,\n",
    "        rerank_model_path:str='damo/nlp_rom_passage-ranking_chinese-base',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param node_list: chunk组成的node的列表\n",
    "        param hybrid_config: 混合检索器的配置，长度为3的bool列表，三个位置分别表示是否使用关键词表检索器、稠密向量检索器、稀疏BM25检索器\n",
    "        param embed_model_path: 嵌入模型地址\n",
    "        param llm_path: 大模型地址\n",
    "        param topk_total: 多路召回时最大数量\n",
    "        param top_k: 检索器返回个数\n",
    "        param threshold: vector_retriever的返回值门槛\n",
    "        param gpu_id: 使用的GPU编号\n",
    "        param rerank: 是否对多路召回重排\n",
    "        \"\"\"\n",
    "        self.hybrid_config = hybrid_config\n",
    "        self.topk = topk\n",
    "        self.topk_total = topk_total\n",
    "        self.threshold = threshold\n",
    "        self._model_load( embed_model_path, llm_path, gpu_id )\n",
    "        self.nodes = self._get_nodes( node_list )\n",
    "        self.rerank_model_path = rerank_model_path\n",
    "        \n",
    "        # 构建检索器\n",
    "        if sum( hybrid_config ) == 0:\n",
    "            raise Exception( f\"请至少指定一种检索器\" )\n",
    "        if hybrid_config[0]:\n",
    "            keyword_storage_context = StorageContext.from_defaults()\n",
    "            keyword_storage_context.docstore.add_documents(self.nodes)\n",
    "            keyword_index = KeywordTableIndex(self.nodes, storage_context=keyword_storage_context)\n",
    "            self.keyword_retriever = KeywordTableSimpleRetriever( keyword_index)\n",
    "        if hybrid_config[1]:\n",
    "            dense_storage_context = StorageContext.from_defaults()\n",
    "            dense_storage_context.docstore.add_documents(self.nodes)\n",
    "            dense_vector_index = VectorStoreIndex(self.nodes, storage_context=dense_storage_context)\n",
    "            self.dense_retriever = VectorIndexRetriever(index=dense_vector_index, similarity_top_k=self.topk, score = self.threshold)\n",
    "        if hybrid_config[2]:\n",
    "            self.sparse_retriever =  BM25Retriever.from_defaults(\n",
    "                docstore=vector_index.docstore, similarity_top_k=topk, verbose=True,\n",
    "                tokenizer=chinese_tokenizer,\n",
    "                language=\"zh\"\n",
    "            )\n",
    "        if rerank_model_path:\n",
    "            self.reranker = pipeline(task=Tasks.text_ranking, model=rerank_model_path, model_revision='v1.1.0')\n",
    "    \n",
    "    def _model_load( self, embed_model_path, llm_path, gpu_id ):\n",
    "        # 指定GPU\n",
    "        if gpu_id != None:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        # 加载嵌入模型\n",
    "        if Settings.embed_model == None:\n",
    "            embed_model = HuggingFaceEmbedding(\n",
    "                model_name = embed_model_path\n",
    "            )\n",
    "            Settings.embed_model = embed_model\n",
    "        # 加载LLM\n",
    "        if llm_path != None and Settings.llm == None:\n",
    "            llm = HuggingFaceLLM(\n",
    "                model_name = llm_path,\n",
    "                tokenizer_name  = llm_path,\n",
    "                model_kwargs={\"trust_remote_code\":True},\n",
    "                tokenizer_kwargs={\"trust_remote_code\":True}\n",
    "            )\n",
    "            Settings.llm = llm\n",
    "\n",
    "    def _get_nodes( self, node_list ):\n",
    "        nodes = []\n",
    "        for i in range(len(node_list)):\n",
    "            metadata={ }\n",
    "            ids = str( len(nodes) )\n",
    "            node = TextNode( text=node_list[i].text, id_=  ids, metadata=metadata)\n",
    "            nodes.append( node )\n",
    "        return nodes\n",
    "    \n",
    "    def retrieve( self, query:str ):\n",
    "        keyword_results_list, dense_results_list, sparse_results_list  = [], [], []\n",
    "        if self.hybrid_config[0]:\n",
    "            keyword_results = self.keyword_retriever.retrieve( query )\n",
    "            for item in keyword_results:\n",
    "                keyword_results_list.append( item.text )\n",
    "        if self.hybrid_config[1]:\n",
    "            dense_results = self.dense_retriever.retrieve( query )\n",
    "            for item in dense_results:\n",
    "                dense_results_list.append( item.text )\n",
    "        if self.hybrid_config[2]:\n",
    "            sparse_results = self.sparse_retriever.retrieve( query )\n",
    "            for item in sparse_results:\n",
    "                sparse_results_list.append( item.text )\n",
    "        all_results = []\n",
    "        all_results.extend( keyword_results_list )\n",
    "        all_results.extend( dense_results_list )\n",
    "        all_results.extend( sparse_results_list )\n",
    "        all_results = list(set(all_results))\n",
    "        if self.rerank_model_path:\n",
    "            all_results = self.re_rank( query, all_results )\n",
    "        return all_results\n",
    "    \n",
    "    @classmethod\n",
    "    def top_k_indices( cls, nums, k):\n",
    "        if not isinstance(nums, list) or not all(isinstance(x, (int, float)) for x in nums):\n",
    "            raise ValueError(\"nums 必须是一个浮点数列表\")\n",
    "        if not isinstance(k, int) or k <= 0:\n",
    "            raise ValueError(\"k 必须是一个正整数\")\n",
    "        sorted_indices = sorted(range(len(nums)), key=lambda i: nums[i], reverse=True)\n",
    "        return sorted_indices[: min(k, len(nums))]\n",
    "    \n",
    "    def get_rerank_top( self, results_list, results_list_rerank, topk=2):\n",
    "        indexs = Hybrid_Retriever.top_k_indices( results_list_rerank, topk )\n",
    "        ret_list = []\n",
    "        for i in range( len(indexs) ):\n",
    "            ret_list.append( results_list[indexs[i]]  )\n",
    "        return ret_list\n",
    "    \n",
    "    def re_rank( self, query, all_results ):\n",
    "        _inputs = {\n",
    "            'source_sentence': [query],\n",
    "            'sentences_to_compare': all_results\n",
    "        }\n",
    "        results_list_rerank = self.reranker(input=_inputs)['scores'] if len(all_results)>0 else []\n",
    "        results = self.get_rerank_top( all_results, results_list_rerank, self.topk_total)\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = Hybrid_Retriever( node_list, [0,1,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"沈国松相关的案件\"\n",
    "results = hybrid_retriever.retrieve( query )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
