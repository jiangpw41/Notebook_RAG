<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>RAGç³»åˆ—ï¼šæ–‡æœ¬å—å‘é‡åŒ–</h1>

ğŸ’¡ å°†æ¯ä¸ªchunkåµŒå…¥ï¼ˆembeddingï¼‰ä¸ºå‘é‡ï¼Œä¾¿äºåç»­æ£€ç´¢ã€‚å¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬ TF-IDFã€Word2Vecã€BERTç­‰é¢„è®­ç»ƒæ¨¡å‹çš„å‘é‡è¡¨ç¤ºã€‚

å‘é‡åŒ–å®é™…ä¸Šæ˜¯å¯¹æ¯ä¸ªè‡ªç„¶è¯­è¨€æ–‡æœ¬è¿›è¡Œembeddingï¼Œä¸»è¦æœ‰ï¼š
- Sparse Embeddingï¼šå…³é”®è¯åŒ¹é…ï¼ŒåŸºäºè¯é¢‘ç»Ÿè®¡
- Dense Embeddingï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ŒåŸºäºç¥ç»ç½‘ç»œ

# 1.Sparse Embedding
ä¾èµ–äºå¯¹æŸ¥è¯¢å’Œæ–‡æ¡£çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…æ¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œé€šè¿‡å€’æ’ç´¢å¼•ç­‰ä¼ ç»Ÿæ–¹æ³•å®ç°ï¼ŒæŠ€æœ¯æˆç†Ÿï¼Œè®¡ç®—æˆæœ¬ä½ã€‚ä½†Sparseæ£€ç´¢ä»…ä¾èµ–äºè¯çš„å‡ºç°ï¼Œæ— æ³•ç†è§£è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹äºåŒä¹‰è¯æˆ–å˜ä½“çš„æŸ¥è¯¢æ— æ³•æ­£ç¡®åŒ¹é…ã€‚å‘é‡ç»´åº¦ç­‰äºæ•´ä¸ªè¯­æ–™åº“çš„è¯æ±‡é‡ã€‚

## 1.1 TF-IDF

### 1.1.1 å…¬å¼
TF-IDFé€šè¿‡è®¡ç®—è¯é¢‘ï¼ˆTFï¼‰å’Œé€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰çš„ä¹˜ç§¯æ¥è¡¡é‡å…³é”®è¯çš„é‡è¦æ€§ã€‚æœ€ç»ˆå½¢æˆä¸€ä¸ªshape = [doc_nums, vocab_size]çš„ç¨€ç–çŸ©é˜µï¼Œå­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„ç¨€ç–å‘é‡ã€‚å…¶ä¸­æ¯ä¸ªå‘é‡çš„é•¿åº¦ä¸ºæ–‡æ¡£è¯è¡¨ï¼Œå‘é‡å€¼è¡¨ç¤ºè¯¥æ–‡æ¡£ä¸­è¯¥è¯çš„IF-IDFå€¼ã€‚

- **TF-IDFå…¬å¼**
$$
\text{TF-IDF}(term,doc)=TF(term,doc)Ã—IDF(term)
$$

- **TFï¼ˆTerm Frequencyï¼‰**ï¼šè¡¨ç¤ºè¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„é¢‘ç‡ã€‚
$$
\text{TF}(term, doc) = \frac{docä¸­å•è¯termçš„æ•°é‡}{ docä¸­æ‰€æœ‰å•è¯çš„æ•°é‡}
$$

- **IDFï¼ˆInverse Document Frequencyï¼‰**ï¼šè¡¡é‡è¯çš„ç¨€æœ‰ç¨‹åº¦ï¼Œç½•è§è¯æƒé‡æ›´é«˜ã€‚
$$
\text{IDF}(term) = \log \frac{æ–‡æ¡£æ€»æ•°}{1 + \text{åŒ…å«å•è¯termçš„æ–‡æ¡£æ•°}}
$$

### 1.1.2 ä»£ç 
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# ç¤ºä¾‹ Chunk æ–‡æœ¬
chunks = [
    "ChatGPT is a large language model developed by OpenAI.",
    "FAISS is an efficient similarity search library for vector retrieval.",
    "BM25 is a ranking function used in search engines and information retrieval.",
]

vectorizer = TfidfVectorizer()
# ç¨€ç–çŸ©é˜µshape = (3, vocab_size)
sparse_vectors = vectorizer.fit_transform( chunks ) 
```

### 1.1.3 æ£€ç´¢è¿‡ç¨‹
ä¸€èˆ¬åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š
1. **é¢„å¤„ç†å’Œå‘é‡åŒ–**ï¼šé¦–å…ˆéœ€è¦å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ã€å»é™¤åœç”¨è¯ç­‰å¤„ç†ï¼Œä¹‹åè®¡ç®—æ¯ä¸ªè¯åœ¨æ–‡æ¡£ä¸­çš„ TF å’Œ IDF å€¼ï¼Œå¹¶ç”Ÿæˆ è¯å’Œæ–‡æ¡£çš„TF-IDF å‘é‡ã€‚
2. **æŸ¥è¯¢å¤„ç†**ï¼šç”¨æˆ·è¾“å…¥æŸ¥è¯¢ï¼ˆqueryï¼‰ï¼Œå°†å…¶è½¬æ¢æˆ TF-IDF å‘é‡ã€‚
3. **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šç”¨æŸ¥è¯¢çš„ TF-IDF å‘é‡ä¸æ¯ä¸ªæ–‡æ¡£çš„ TF-IDF å‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé€šå¸¸ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
4. **æ’åº**ï¼šæ ¹æ®è®¡ç®—çš„ç›¸ä¼¼åº¦å€¼å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ 3 ä¸ªæ–‡æ¡£ï¼š

1. `"ChatGPT is a large language model developed by OpenAI."`
2. `"FAISS is an efficient similarity search library for vector retrieval."`
3. `"BM25 is a ranking function used in search engines and information retrieval."`

ç”¨æˆ·æŸ¥è¯¢ï¼š`"search engine"`

é¦–å…ˆè®¡ç®— TF å’Œ IDF å€¼ï¼Œæ„å»ºæ¯ä¸ªæ–‡æ¡£çš„ TF-IDF å‘é‡ã€‚å‡è®¾å·²ç»è®¡ç®—å‡ºå¦‚ä¸‹çš„ TF-IDF çŸ©é˜µï¼ˆé™¤æœ€åä¸€åˆ—ï¼‰ï¼š

| è¯é¡¹ (Term) | æ–‡æ¡£ 1 (Chunk 1) | æ–‡æ¡£ 2 (Chunk 2) | æ–‡æ¡£ 3 (Chunk 3) | query |
| :---- | :----: | :---: | :---: | :---: |
|chatgpt| 0.57   |0.00| 0.00 | 0.00 |
|faiss	|0.00|0.57|0.00|0.00 |
|search	|0.00|0.41|0.57|0.57 |
|ranking|0.00|0.00|0.57|0.00 |
|engine|0.00|0.41|0.00|0.41 |
|retrieval|0.00|0.41|0.41|0.00 |

å¯¹äºæŸ¥è¯¢ `"search engine"`ï¼šå¯¹äºæŸ¥è¯¢ä¸­çš„æ¯ä¸ªè¯é¡¹ï¼Œ**è¯é¢‘**ï¼ˆTFï¼‰å°±æ˜¯è¯¥è¯é¡¹åœ¨æŸ¥è¯¢ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œæ­¤å¤–ï¼Œæˆ‘ä»¬æŸ¥æ‰¾å®ƒåœ¨ **æ•´ä¸ªè¯­æ–™åº“** ä¸­çš„ **é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰**ï¼Œå³æ¯ä¸ªè¯é¡¹åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„ç¨€æœ‰ç¨‹åº¦ã€‚

- **search** çš„ TF-IDFï¼š0.57
- **engine** çš„ TF-IDFï¼š0.41

æŸ¥è¯¢å‘é‡çš„ TF-IDF å€¼æ˜¯ï¼š[0.57, 0.41]

æœ€åå°†æŸ¥è¯¢å‘é‡å¯¹é½åˆ°è¯è¡¨ç©ºé—´ï¼Œå½¢æˆå’Œæ–‡æ¡£å‘é‡é•¿åº¦ä¸€è‡´çš„å‘é‡ï¼ˆä¸Šè¡¨æœ€åä¸€åˆ—ï¼‰ã€‚è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ¯ä¸ªæ–‡æ¡£å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚æ’åºç›¸ä¼¼åº¦å¾—åˆ†åï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç»™ç”¨æˆ·ã€‚


## 1.2 BM25ï¼šä¸ç›´æ¥ç”Ÿæˆå‘é‡
BM25 æ˜¯ TF-IDF çš„æ”¹è¿›ç‰ˆï¼Œå¼•å…¥äº†è¯é¢‘é¥±å’Œå’Œæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼Œé€‚ç”¨äºæœç´¢æ’åã€‚M25 ä¸ä¼šç›´æ¥åƒ TF-IDF é‚£æ ·ä¸ºæ¯ä¸ª Chunk ç”Ÿæˆå›ºå®šçš„ç¨€ç–å‘é‡ï¼Œè€Œæ˜¯æ ¹æ®æŸ¥è¯¢ï¼ˆqueryï¼‰è®¡ç®—å¾—åˆ†ã€‚å®ƒçš„æœ¬è´¨æ˜¯ä¸€ä¸ªæ£€ç´¢æ¨¡å‹ï¼Œè€Œéå‘é‡åŒ–æ–¹æ³•ã€‚

## 1.2.1 å…¬å¼
BM25å…¬å¼å¦‚ä¸‹
$$
\text{BM25}(t, d) = IDF(t) \times \frac{\text{TF}(t, d) \cdot (k_1 + 1)}{\text{TF}(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avg}_{dl}})}
$$

å…¶ä¸­

- $k_1$ æ§åˆ¶è¯é¢‘å½±å“ï¼Œé€šå¸¸å–å€¼1.2-2ã€‚
- $b$æ˜¯æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ç¨€ç–ï¼Œå¸¸ä¸º0.7ã€‚
- $|d|$è¡¨ç¤ºå½“å‰æ–‡æ¡£é•¿åº¦
- $avg_{dl}$æ˜¯æ‰€æœ‰æ–‡æ¡£å¹³å‡é•¿åº¦

## 1.2.2 ä»£ç 
```python
from rank_bm25 import BM25Okapi
import numpy as np

# å‡è®¾æœ‰ 3 ä¸ª Chunk
chunks = [
    "ChatGPT is a large language model developed by OpenAI.",
    "FAISS is an efficient similarity search library for vector retrieval.",
    "BM25 is a ranking function used in search engines and information retrieval.",
]

# é¢„å¤„ç†
tokenized_chunks = [chunk.lower().split() for chunk in chunks]

# æ„å»º BM25 æ¨¡å‹
bm25 = BM25Okapi( tokenized_chunks )

# è®¡ç®—æ¯ä¸ª Chunk çš„ç¨€ç–å‘é‡
sparse_vectors_bm25 = np.array([bm25.get_scores(chunk) for chunk in tokenized_chunks])
print(sparse_vectors_bm25)  # ç»“æœï¼šBM25 è¯„åˆ†çŸ©é˜µï¼Œè¡¨ç¤ºæ¯ä¸ª Chunk åœ¨æ•´ä¸ªæ–‡æ¡£é›†ä¸­çš„é‡è¦æ€§ã€‚

# æŸ¥è¯¢
query = "search engine ranking"
tokenized_query = query.lower().split()

# è®¡ç®— Query ä¸ Chunks çš„ BM25 ç›¸å…³æ€§å¾—åˆ†
scores = bm25.get_scores(tokenized_query)
print(scores)  # è¾“å‡º BM25 ç›¸å…³æ€§åˆ†æ•°

```
## 1.2.3 æ£€ç´¢è¿‡ç¨‹
BM25 åœ¨å†…éƒ¨ç´¢å¼•è¿‡ç¨‹ä¸­ä¸ä¼šå°†æ¯ä¸ª Chunk å˜ä¸ºå›ºå®šå‘é‡ï¼Œè€Œæ˜¯æ„å»ºå€’æ’ç´¢å¼•ï¼ˆInverted Indexï¼‰ã€‚

BM25 ä¸æ˜¯ä¸ºæ¯ä¸ª Chunk è®¡ç®—ä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼Œè€Œæ˜¯**å»ºç«‹ä¸€ä¸ªå€’æ’ç´¢å¼•ï¼ˆInverted Indexï¼‰**ï¼Œè®°å½•**æ¯ä¸ªè¯åœ¨ä¸åŒ Chunkï¼ˆæ–‡æ¡£ï¼‰ä¸­çš„å‡ºç°æƒ…å†µ**ã€‚

**å€’æ’ç´¢å¼•ç»“æ„ç¤ºä¾‹**ï¼ˆå‡è®¾æœ‰ 3 ä¸ª Chunksï¼‰ï¼š

å€’æ’ç´¢å¼•ï¼ˆInverted Indexï¼‰æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨**è¯ï¼ˆtermï¼‰åˆ°æ–‡æ¡£ï¼ˆchunksï¼‰çš„æ˜ å°„å…³ç³»**ã€‚å®ƒåŒ…æ‹¬ï¼š

1. **è¯å…¸ï¼ˆDictionaryï¼‰**ï¼šå­˜å‚¨æ‰€æœ‰åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„è¯é¡¹ï¼ˆtermï¼‰ã€‚
2. **å€’æ’åˆ—è¡¨ï¼ˆPosting Listï¼‰**ï¼šå¯¹äºæ¯ä¸ªè¯é¡¹ï¼Œå­˜å‚¨åŒ…å«è¯¥è¯çš„æ–‡æ¡£ ID ä»¥åŠè¯é¢‘ï¼ˆTFï¼‰ã€æ–‡æ¡£é•¿åº¦ç­‰ä¿¡æ¯ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ 3 ä¸ª `chunks`ï¼š

1. `"ChatGPT is a large language model developed by OpenAI."`
2. `"FAISS is an efficient similarity search library for vector retrieval."`
3. `"BM25 is a ranking function used in search engines and information retrieval."`

æˆ‘ä»¬å¯¹è¿™ 3 ä¸ª Chunks æ„å»ºå€’æ’ç´¢(æ¯ä¸ªè¯å‡ºç°åœ¨é‚£äº›æ–‡æ¡£ä¸­åŠå…¶é¢‘ç‡TF)å¼•å¦‚ä¸‹:

| è¯é¡¹ (Term) | å€’æ’åˆ—è¡¨ (Posting List) |
| :---- | :----: |
|chatgpt	|{1: TF=1}|
|faiss	|{2: TF=1}|
|search	|{2: TF=1}, {3: TF=1}|
|ranking	|{3: TF=1}|
|model	|{1: TF=1}|
|retrieval	|{2: TF=1}, {3: TF=1}|

åœ¨ BM25 çš„ç´¢å¼•è¿‡ç¨‹ä¸­ï¼Œå®ƒä¸ä¼šç›´æ¥å­˜å‚¨ä¸€ä¸ª N ç»´å‘é‡ï¼Œè€Œæ˜¯**å­˜å‚¨æ¯ä¸ªå•è¯åœ¨å“ªäº› Chunkï¼ˆæ–‡æ¡£ï¼‰ä¸­å‡ºç°ã€å‡ºç°æ¬¡æ•°ç­‰ä¿¡æ¯**ã€‚å½“ç”¨æˆ·è¾“å…¥æŸ¥è¯¢ï¼ˆqueryï¼‰æ—¶ï¼ŒBM25 **åˆ©ç”¨å€’æ’ç´¢å¼•é«˜æ•ˆæŸ¥æ‰¾åŒ…å«æŸ¥è¯¢è¯çš„ Chunk**ï¼Œç„¶åæ ¹æ® BM25 å…¬å¼è®¡ç®—ç›¸å…³æ€§å¾—åˆ†ã€‚
**è®¡ç®—æ­¥éª¤ï¼š**
1. æŸ¥è¯¢ `"search engine ranking"`
2. æ ¹æ®å€’æ’ç´¢å¼•æ‰¾åˆ°åŒ…å« `"search"`ã€`"engine"`ã€`"ranking"` çš„ Chunk
3. è®¡ç®—æ¯ä¸ª Chunk çš„ BM25 ç›¸å…³æ€§åˆ†æ•°ï¼ˆç´¯åŠ æŸ¥è¯¢ä¸­æ‰€æœ‰Itemå’Œæ¯ä¸ªæ–‡æ¡£çš„BM25åˆ†æ•°ä½œä¸ºæŸ¥è¯¢æ€»ä½“å’Œæ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼‰
4. è¿”å›æ’åºåçš„ç»“æœ


# 2. Dense Embedding
é€šè¿‡å°†æ–‡æœ¬ï¼ˆæŸ¥è¯¢å’Œæ–‡æ¡£ï¼‰æ˜ å°„åˆ°ç¨ å¯†çš„å‘é‡ç©ºé—´ä¸­ï¼Œåˆ©ç”¨å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢ã€‚ç¨ å¯†æ£€ç´¢çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨è¯å‘é‡æ¨¡å‹å­¦ä¹ æ–‡æœ¬çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œæ•æ‰æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢çš„è¯æ±‡åŒ¹é…ã€‚

## 2.1 è¯å‘é‡æ¨¡å‹é€‰æ‹©
è¯å‘é‡æ¨¡å‹é€‰æ‹©ï¼šå³Embedingæ¨¡å‹ï¼Œå°†æŸ¥è¯¢å’Œæ–‡æ¡£ä¸¤è€…æ˜ å°„ä¸ºé•¿åº¦ç»Ÿä¸€çš„ç¨ å¯†å‘é‡ï¼ˆå¦‚512ç»´ï¼‰

- **å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹**ï¼šå¦‚æœè¿½æ±‚å¿«é€Ÿå®ç°ä¸”ä»»åŠ¡é¢†åŸŸè¾ƒé€šç”¨ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒEmbeddingæ¨¡å‹
    - å¦‚Transformer-basedæ¨¡å‹ï¼ˆBERTã€RoBERTaã€T5ç­‰ï¼‰å’Œæ—©æœŸçš„è¯å‘é‡æ¨¡å‹ï¼ˆWord2Vecã€Gloveç­‰ï¼‰
    - ç‰¹ç‚¹ï¼šå·²åœ¨å¤§é‡è¯­æ–™åº“ä¸Šå®Œæˆäº†é¢„è®­ç»ƒï¼Œå…·å¤‡ç›¸å¯¹é€šç”¨çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
    - å¸¸ç”¨ï¼štransformerè¯å‘é‡æ¨¡å‹ï¼ŒBERTã€RoBERTaã€DistilBERTã€Sentence-BERTï¼ˆSBERTï¼‰ã€text-embedding-ada-002  (OpenAIçš„embeddingæ¨¡å‹)
- **é’ˆå¯¹æ£€ç´¢ä»»åŠ¡å¾®è°ƒ**ï¼šå¦‚æœéœ€è¦æ›´é«˜çš„ä¸“ä¸šæ€§ï¼Œè€ƒè™‘åˆ°ä»å¤´é¢„è®­ç»ƒçš„æˆæœ¬ï¼ˆæ•°æ®é‡ã€æ—¶é—´ã€ç¡¬ä»¶ç­‰ï¼‰ï¼Œä¸€èˆ¬å¯¹ç°æœ‰è¯å‘é‡æ¨¡å‹è¿›è¡Œå¾®è°ƒ
    - æ•°æ®é›†å‡†å¤‡ï¼šéœ€è¦æ ‡æ³¨å¥½çš„æŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼ˆæ­£æ ·æœ¬ï¼‰ä»¥åŠå¯èƒ½çš„è´Ÿæ ·æœ¬ï¼Œä¾‹å¦‚ï¼ŒMS MARCOæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æ£€ç´¢ä»»åŠ¡æ•°æ®é›†ã€‚
    - é€‰æ‹©æŸå¤±å‡½æ•°ï¼š
        - å¯¹æ¯”æŸå¤±ï¼ˆContrastive Lossï¼‰ï¼šä½¿æ­£æ ·æœ¬çš„å‘é‡è·ç¦»æ›´è¿‘ï¼Œè´Ÿæ ·æœ¬çš„å‘é‡è·ç¦»æ›´è¿œã€‚
        - ä¸‰å…ƒç»„æŸå¤±ï¼ˆTriplet Lossï¼‰ï¼šç»™å®šä¸€ä¸ªé”šç‚¹ï¼ˆæŸ¥è¯¢ï¼‰ã€æ­£æ ·æœ¬ï¼ˆç›¸å…³æ–‡æ¡£ï¼‰å’Œè´Ÿæ ·æœ¬ï¼ˆä¸ç›¸å…³æ–‡æ¡£ï¼‰ï¼Œä¼˜åŒ–é”šç‚¹ä¸æ­£æ ·æœ¬çš„è·ç¦»å°äºé”šç‚¹ä¸è´Ÿæ ·æœ¬çš„è·ç¦»ã€‚
    - ä½¿ç”¨æ¡†æ¶ï¼ˆå¦‚Hugging Faceçš„`Trainer`æˆ–ç›´æ¥ä½¿ç”¨PyTorchï¼‰è¿›è¡Œå¾®è°ƒã€‚
- **é’ˆå¯¹ä¸“ä¸šé¢†åŸŸä»å¤´é¢„è®­ç»ƒ**ï¼šéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œé¢†åŸŸæ•°æ®ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿
    - å‡†å¤‡å¤§è§„æ¨¡é¢†åŸŸæ•°æ®é›†ã€‚
    - è®¾è®¡æ¨¡å‹æ¶æ„ï¼ˆå¦‚åŸºäºTransformerçš„ç¼–ç å™¨ï¼‰ã€‚
        - å¦‚DPRï¼ˆDense Passage Retrievalï¼‰ï¼Œä¸“é—¨ä¸ºé—®ç­”ä»»åŠ¡è®¾è®¡çš„ç¨ å¯†æ£€ç´¢æ¨¡å‹ï¼Œä½¿ç”¨BERTä½œä¸ºç¼–ç å™¨ï¼Œåˆ†åˆ«ç¼–ç æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œè®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ç›¸å…³æŸ¥è¯¢-æ–‡æ¡£å¯¹çš„å‘é‡è·ç¦»ã€‚
    - ä½¿ç”¨é¢†åŸŸæ•°æ®ä»å¤´è®­ç»ƒæ¨¡å‹ã€‚

## 2.2 ä»£ç ç¤ºä¾‹

### 2.2.1 é™æ€è¯å‘é‡

Word2vecï¼ŒGloveï¼ŒFastTextç­‰é™æ€è¯å‘é‡æ¨¡å‹ï¼ˆå¯¹è¯çš„embeddingä¸è€ƒè™‘ä¸Šä¸‹æ–‡ï¼Œæ’å®šï¼‰ç”±gensimåº“æ”¯æŒï¼Œé€‚ç”¨äºç®€å•æ–‡æœ¬å’Œç‰¹å®šé¢†åŸŸï¼Œä¼˜åŠ¿æ˜¯ä½æˆæœ¬å¿«é€Ÿå“åº”ã€‚
```python 
import gensim.downloader as api

# åŠ è½½é¢„è®­ç»ƒçš„ Word2Vec æ¨¡å‹ï¼šä»APIæˆ–è€…æœ¬åœ°
static_model = api.load("word2vec-google-news-300")
static_model = gensim.models.KeyedVectors.load_word2vec_format('path/to/word2vec_model.bin', binary=True)
static_model = gensim.models.KeyedVectors.load_word2vec_format('path/to/glove_model.txt', binary=False)
static_model = model = gensim.models.fasttext.load_facebook_vectors('path/to/fasttext_model.bin')

# ä½¿ç”¨æ¨¡å‹è·å–å•è¯çš„è¯å‘é‡
word_vector = word2vec_model["king"]
print(f"Word2Vec vector for 'king': {word_vector}")
```
### 2.2.2 åŠ¨æ€è¯å‘é‡

åŠ¨æ€è¯å‘é‡å¯¹åŒä¸€ä¸ªè¯ä¼šæ ¹æ®ä¸Šä¸‹æ–‡ä¸åŒç»™äºˆä¸åŒçš„embeddingï¼Œåœ¨Transformerå‡ºç°ä¹‹å‰ï¼Œå¤šåŸºäºLSTMã€‚ä½†ç›®å‰ä»¥Transformeræ¨¡å‹ä¸ºä¸»ï¼Œå¦‚BERTã€‚é€šå¸¸ç”± Hugging Face æä¾›çš„ transformers åº“æ”¯æŒ
```python
from transformers import BertTokenizer, BertModel
import torch

# åŠ è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# ä½¿ç”¨ BERT åˆ†è¯å™¨ç¼–ç æ–‡æœ¬
inputs = tokenizer("King is a powerful word", return_tensors="pt")

# è·å– BERT æ¨¡å‹çš„è¾“å‡º
with torch.no_grad():
    outputs = model(**inputs)

# è¾“å‡ºçš„æœ€åä¸€å±‚éšçŠ¶æ€æ˜¯è¯å‘é‡
last_hidden_states = outputs.last_hidden_state

# è·å–â€œKingâ€çš„è¯å‘é‡ï¼ˆå‡è®¾æ˜¯è¾“å…¥å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯ï¼‰
king_vector = last_hidden_states[0][0]

print(f"BERT vector for 'King': {king_vector}")
```