
💡 将每个chunk嵌入（embedding）为向量，便于后续检索。常用的方法包括 TF-IDF、Word2Vec、BERT等预训练模型的向量表示。

向量化实际上是对每个自然语言文本进行embedding，主要有：
- Sparse Embedding：关键词匹配，基于词频统计
- Dense Embedding：语义相似度，基于神经网络

# 1.Sparse Embedding
依赖于对查询和文档的关键词进行匹配来检索相关信息，通过倒排索引等传统方法实现，技术成熟，计算成本低。但Sparse检索仅依赖于词的出现，无法理解词语之间的语义关系，这可能导致对于同义词或变体的查询无法正确匹配。向量维度等于整个语料库的词汇量。

## 1.1 TF-IDF

### 1.1.1 公式
TF-IDF通过计算词频（TF）和逆文档频率（IDF）的乘积来衡量关键词的重要性。最终形成一个shape = [doc_nums, vocab_size]的稀疏矩阵，存储每个文档的稀疏向量。其中每个向量的长度为文档词表，向量值表示该文档中该词的IF-IDF值。

- **TF-IDF公式**
$$
\text{TF-IDF}(term,doc)=TF(term,doc)×IDF(term)
$$

- **TF（Term Frequency）**：表示词在文本中出现的频率。
$$
\text{TF}(term, doc) = \frac{doc中单词term的数量}{ doc中所有单词的数量}
$$

- **IDF（Inverse Document Frequency）**：衡量词的稀有程度，罕见词权重更高。
$$
\text{IDF}(term) = \log \frac{文档总数}{1 + \text{包含单词term的文档数}}
$$

### 1.1.2 代码
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 示例 Chunk 文本
chunks = [
    "ChatGPT is a large language model developed by OpenAI.",
    "FAISS is an efficient similarity search library for vector retrieval.",
    "BM25 is a ranking function used in search engines and information retrieval.",
]

vectorizer = TfidfVectorizer()
# 稀疏矩阵shape = (3, vocab_size)
sparse_vectors = vectorizer.fit_transform( chunks ) 
```

### 1.1.3 检索过程
一般包括以下几个步骤：
1. **预处理和向量化**：首先需要对文档进行分词、去除停用词等处理，之后计算每个词在文档中的 TF 和 IDF 值，并生成 词和文档的TF-IDF 向量。
2. **查询处理**：用户输入查询（query），将其转换成 TF-IDF 向量。
3. **相似度计算**：用查询的 TF-IDF 向量与每个文档的 TF-IDF 向量计算相似度，通常使用余弦相似度。
4. **排序**：根据计算的相似度值对文档进行排序，返回最相关的文档。

假设我们有 3 个文档：

1. `"ChatGPT is a large language model developed by OpenAI."`
2. `"FAISS is an efficient similarity search library for vector retrieval."`
3. `"BM25 is a ranking function used in search engines and information retrieval."`

用户查询：`"search engine"`

首先计算 TF 和 IDF 值，构建每个文档的 TF-IDF 向量。假设已经计算出如下的 TF-IDF 矩阵（除最后一列）：

| 词项 (Term) | 文档 1 (Chunk 1) | 文档 2 (Chunk 2) | 文档 3 (Chunk 3) | query |
| :---- | :----: | :---: | :---: | :---: |
|chatgpt| 0.57   |0.00| 0.00 | 0.00 |
|faiss	|0.00|0.57|0.00|0.00 |
|search	|0.00|0.41|0.57|0.57 |
|ranking|0.00|0.00|0.57|0.00 |
|engine|0.00|0.41|0.00|0.41 |
|retrieval|0.00|0.41|0.41|0.00 |

对于查询 `"search engine"`：对于查询中的每个词项，**词频**（TF）就是该词项在查询中出现的次数，此外，我们查找它在 **整个语料库** 中的 **逆文档频率（IDF）**，即每个词项在所有文档中的稀有程度。

- **search** 的 TF-IDF：0.57
- **engine** 的 TF-IDF：0.41

查询向量的 TF-IDF 值是：[0.57, 0.41]

最后将查询向量对齐到词表空间，形成和文档向量长度一致的向量（上表最后一列）。计算查询向量与每个文档向量的余弦相似度。排序相似度得分后，返回最相关的文档给用户。


## 1.2 BM25：不直接生成向量
BM25 是 TF-IDF 的改进版，引入了词频饱和和文档长度归一化，适用于搜索排名。M25 不会直接像 TF-IDF 那样为每个 Chunk 生成固定的稀疏向量，而是根据查询（query）计算得分。它的本质是一个检索模型，而非向量化方法。

## 1.2.1 公式
BM25公式如下
$$
\text{BM25}(t, d) = IDF(t) \times \frac{\text{TF}(t, d) \cdot (k_1 + 1)}{\text{TF}(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avg}_{dl}})}
$$

BM25在TF-IDF的基础上做了一些改进。
- 控制高频词影响：
	- 当TF过大时，词频重要性被削弱。
	- $k_1$ 控制词频影响，通常取值1.2-2。
- 文档长度归一化：
	- $|d|$表示当前文档长度，$avg_{dl}$是所有文档平均长度
	- 两者比值可以避免较长文档因为包含更多词而占优势
	-  $b$是文档长度归一化系数，常为0.7。

## 1.2.2 代码
```python
from rank_bm25 import BM25Okapi
import numpy as np

# 假设有 3 个 Chunk
chunks = [
    "ChatGPT is a large language model developed by OpenAI.",
    "FAISS is an efficient similarity search library for vector retrieval.",
    "BM25 is a ranking function used in search engines and information retrieval.",
]

# 预处理
tokenized_chunks = [chunk.lower().split() for chunk in chunks]

# 构建 BM25 模型
bm25 = BM25Okapi( tokenized_chunks )

# 计算每个 Chunk 的稀疏向量
sparse_vectors_bm25 = np.array([bm25.get_scores(chunk) for chunk in tokenized_chunks])
print(sparse_vectors_bm25)  # 结果：BM25 评分矩阵，表示每个 Chunk 在整个文档集中的重要性。

# 查询
query = "search engine ranking"
tokenized_query = query.lower().split()

# 计算 Query 与 Chunks 的 BM25 相关性得分
scores = bm25.get_scores(tokenized_query)
print(scores)  # 输出 BM25 相关性分数

```
## 1.2.3 检索过程
BM25 在内部索引过程中不会将每个 Chunk 变为固定向量，而是构建倒排索引（Inverted Index）。

BM25 不是为每个 Chunk 计算一个固定的向量，而是**建立一个倒排索引（Inverted Index）**，记录**每个词在不同 Chunk（文档）中的出现情况**。

**倒排索引结构示例**（假设有 3 个 Chunks）：

倒排索引（Inverted Index）是一种数据结构，用于存储**词（term）到文档（chunks）的映射关系**。它包括：

1. **词典（Dictionary）**：存储所有在文档中出现的词项（term）。
2. **倒排列表（Posting List）**：对于每个词项，存储包含该词的文档 ID 以及词频（TF）、文档长度等信息。

假设我们有 3 个 `chunks`：

1. `"ChatGPT is a large language model developed by OpenAI."`
2. `"FAISS is an efficient similarity search library for vector retrieval."`
3. `"BM25 is a ranking function used in search engines and information retrieval."`

我们对这 3 个 Chunks 构建倒排索(每个词出现在那些文档中及其频率TF)引如下:

| 词项 (Term) | 倒排列表 (Posting List) |
| :---- | :----: |
|chatgpt	|{1: TF=1}|
|faiss	|{2: TF=1}|
|search	|{2: TF=1}, {3: TF=1}|
|ranking	|{3: TF=1}|
|model	|{1: TF=1}|
|retrieval	|{2: TF=1}, {3: TF=1}|

在 BM25 的索引过程中，它不会直接存储一个 N 维向量，而是**存储每个单词在哪些 Chunk（文档）中出现、出现次数等信息**。当用户输入查询（query）时，BM25 **利用倒排索引高效查找包含查询词的 Chunk**，然后根据 BM25 公式计算相关性得分。
**计算步骤：**
1. 查询 `"search engine ranking"`
2. 根据倒排索引找到包含 `"search"`、`"engine"`、`"ranking"` 的 Chunk
3. 计算每个 Chunk 的 BM25 相关性分数（累加查询中所有Item和每个文档的BM25分数作为查询总体和文档的相似度）
4. 返回排序后的结果


# 2. Dense Embedding
通过将文本（查询和文档）映射到稠密的向量空间中，利用向量之间的相似度进行检索。稠密检索的核心思想是利用词向量模型学习文本的语义表示，从而捕捉查询和文档之间的语义相关性，而不仅仅是表面的词汇匹配。

## 2.1 词向量模型选择
词向量模型选择：即Embeding模型，将查询和文档两者映射为长度统一的稠密向量（如512维）

- **开箱即用的预训练模型**：如果追求快速实现且任务领域较通用，可以直接使用现有的预训练Embedding模型
    - 如Transformer-based模型（BERT、RoBERTa、T5等）和早期的词向量模型（Word2Vec、Glove等）
    - 特点：已在大量语料库上完成了预训练，具备相对通用的语义理解能力，可以直接使用。
    - 常用：transformer词向量模型，BERT、RoBERTa、DistilBERT、Sentence-BERT（SBERT）、text-embedding-ada-002  (OpenAI的embedding模型)
- **针对检索任务微调**：如果需要更高的专业性，考虑到从头预训练的成本（数据量、时间、硬件等），一般对现有词向量模型进行微调
    - 数据集准备：需要标注好的查询-文档对（正样本）以及可能的负样本，例如，MS MARCO是一个常用的检索任务数据集。
    - 选择损失函数：
        - 对比损失（Contrastive Loss）：使正样本的向量距离更近，负样本的向量距离更远。
        - 三元组损失（Triplet Loss）：给定一个锚点（查询）、正样本（相关文档）和负样本（不相关文档），优化锚点与正样本的距离小于锚点与负样本的距离。
    - 使用框架（如Hugging Face的`Trainer`或直接使用PyTorch）进行微调。
- **针对专业领域从头预训练**：需要大量计算资源和领域数据，训练时间较长
    - 准备大规模领域数据集。
    - 设计模型架构（如基于Transformer的编码器）。
        - 如DPR（Dense Passage Retrieval），专门为问答任务设计的稠密检索模型，使用BERT作为编码器，分别编码查询和文档，训练目标是最小化相关查询-文档对的向量距离。
    - 使用领域数据从头训练模型。

## 2.2 代码示例

### 2.2.1 静态词向量

Word2vec，Glove，FastText等静态词向量模型（对词的embedding不考虑上下文，恒定）由gensim库支持，适用于简单文本和特定领域，优势是低成本快速响应。
```python 
import gensim.downloader as api

# 加载预训练的 Word2Vec 模型：从API或者本地
static_model = api.load("word2vec-google-news-300")
static_model = gensim.models.KeyedVectors.load_word2vec_format('path/to/word2vec_model.bin', binary=True)
static_model = gensim.models.KeyedVectors.load_word2vec_format('path/to/glove_model.txt', binary=False)
static_model = model = gensim.models.fasttext.load_facebook_vectors('path/to/fasttext_model.bin')

# 使用模型获取单词的词向量
word_vector = word2vec_model["king"]
print(f"Word2Vec vector for 'king': {word_vector}")
```
### 2.2.2 动态词向量

动态词向量对同一个词会根据上下文不同给予不同的embedding，在Transformer出现之前，多基于LSTM。但目前以Transformer模型为主，如BERT。通常由 Hugging Face 提供的 transformers 库支持
```python
from transformers import BertTokenizer, BertModel
import torch

# 加载预训练的 BERT 模型和分词器
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# 使用 BERT 分词器编码文本
inputs = tokenizer("King is a powerful word", return_tensors="pt")

# 获取 BERT 模型的输出
with torch.no_grad():
    outputs = model(**inputs)

# 输出的最后一层隐状态是词向量
last_hidden_states = outputs.last_hidden_state

# 获取“King”的词向量（假设是输入句子的第一个单词）
king_vector = last_hidden_states[0][0]

print(f"BERT vector for 'King': {king_vector}")
```