<!-- JPW的Markdown笔记模板 v1, 其中的href需要视情更改上级目录href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>RAG系列：数据预处理</h1>

💡 收集专业性文档集，并进行清洗、文本标准化、纠错、分块chunk等。

# 1 预处理策略

对知识库文本的预处理主要包括数据清洗和分块chunking两部分。

## 1.1 清洗与标准化

- **噪声去除（必需）**：
  - 内容过滤：
    - 低质量内容：如HTML标签、特殊字符、无用的标点符号、垃圾文本、重复
    - 敏感内容：涉密、隐私、违法信息等。
  - 文本规范化：
    - 格式规范：统一全角/半角字符、大小写、停用词、多余的空格、日期格式（如“2023-08-20” → “2023年8月20日”）。
    - 段落规范：使用正则表达式修复断句（如“Mr. Smith”避免被错误切分为两句）。
    - 内容规范：如拼写校正等
- **质量提升（可选）**：
  - 实体识别NER与消歧：识别文本中的实体（人名、地点、术语），将实体链接到知识图谱（如Wikidata、DBpedia），消除歧义（如“苹果” → “Apple Inc.”或“水果苹果”）。
  - 语义总结：使用LLMs等模型进行摘要、关键词总结提炼等。
  - metadata元数据：为每个分块添加来源、时间、作者等信息，辅助检索排序

## 1.2 分块chunking
将长文档拆分成更小的块，方便检索模块和生成模块高效地处理。主要包括四种策略
- **基于固定长度分割**：最常见的策略，按照固定token数量切分（例如512），以适合embedding模型输入。如果文档比较简单，可以采用基于固定token数的分割方法，这样的优势是简单高效，但可能导致上下文丢失，特别是跨句子时。
  - **Size**：设置合理的chunk size非常重要，过大可能会导致检索时信息冗余，过小则可能丢失上下文信息。一般来说，推荐的token数在 **200到500个token** 之间
  - **Overlap**：相邻两个chunk之间的重叠部分，即每个chunk的结尾和下一个chunk的开头之间的共享文本。重叠可以帮助模型更好地理解跨块的上下文信息，尤其是在处理长文本时。典型的重叠设置范围在 **20% 到 50%** 之间。重叠过多可能会导致冗余计算，增加处理时间，但有时为避免信息丢失，适度的重叠是非常必要的。
- **基于句子或段落分割**：最常见的策略是将文档分割成一个个完整的句子或段落，然后再合并成chunk，这样可以确保块内的语法结构是完整的。
- **基于语义分割**：这是一种更加高级的分割方式，通过分析句子或段落的语义关系来进行划分，基于句子嵌入相似度动态合并相邻段落，使得每个chunk在语义上尽量独立且完整。这种方法需要更多的计算资源。
- **基于逻辑规则分割**：按章节、段落或表格切分（如Markdown标题分层）。特别的，对于一组专业文档，如法律裁判文书，可以通过一些通用的规则进行分割（开头基本流程、原告诉请、被告辩称、法庭裁决等）

# 2 代码实践