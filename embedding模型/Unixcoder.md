ACL 2022, 中山大学和Microsoft。
一作：Daya Guo，在微软实习时的研究成果，目前在DeepSeek任职，是R1的技术报告一作: https://scholar.google.com/citations?hl=zh-CN&user=gCG4cPYAAAAJ&view_op=list_works&sortby=pubdate

题目：UniXcoder: Unified Cross-Modal Pre-training for Code Representation（代码表示的统一跨模态预训练）。
论文：https://arxiv.org/abs/2203.03850
代码：被合并到微软的CodeBert下：https://github.com/microsoft/CodeBERT

Unified：代码理解和生成使用一个套参数
- 在输入的注释 + AST平铺前，增加一个 Prefix 特殊token（三种），以控制注意力点积获得的矩阵+Mask矩阵的操作。
- \[Enc]前缀则将掩码矩阵的所有元素设置为0，前缀\[Dec]则将掩码的上三角部分设置为−∞以表示每个令牌只能处理自己和之前的令牌，\[E2D]前缀则允许源输入中的令牌相互关注而目标输入中的标记仅关注自身以及源和目标输入中之前的令牌。
Cross-Modal跨模态：模型用两类数据训练：
- 注释：人类对代码的功能解释
- AST信息：通过本文提出的一一映射方法，从树形结构转变为平铺的序列flattened token sequence。

训练目标：三种类型的语言建模任务进行预训练
- MLM掩码语言建模（Devlin等人，2018）
- 单向语言建模unidirectional language modeling（Radford等人，2018年）
- 去噪目标denoising objective（Raffel等人，2019年）

两个预训练任务来学习可以表示代码片段语义的embedding
- 多模态对比学习，它利用AST来增强代码片段嵌入的语义
- 跨模态生成，它利用代码注释来对齐编程语言之间的embedding。

核心创新
- 带有前缀适配器的掩码注意力矩阵mask attention matrices：控制模型的行为，同一套参数通过“<mark style="background: #FF5582A6;">前缀+掩码</mark>”实现三种模式：Encoder / Decoder / Encoder-Decoder。
	- 解决痛点：架构割裂。编码器-解码器模型在自回归任务（如代码补全）上效率低；纯编码器/解码器又无法兼顾代码理解+生成
- 使用<mark style="background: #FF5582A6;">多模态</mark>训练内容：
	- 解决痛点：信息单一。以往工作仅把代码当纯文本，或使用 AST/数据流但任务单一
	- 提出一种一一映射方法将树形AST映射为序列并保留结构信息（可唯一性映射回树形结构）。
	- 提出使用多模态内容（注释、AST）进行对比学习 contrastive learning，学习代码片段的表示；然后使用跨模式生成任务在编程语言之间对齐表示。

在9个数据集上对UniXcoder进行了5个代码相关任务的评估。为了进一步评估代码片段表示的性能，我们还为一个新任务构建了一个数据集，称为<mark style="background: #FF5582A6;">零样本代码到代码搜索</mark> zero-shot code-to-code search。结果表明，我们的模型在大多数任务上都达到了最先进的性能，分析表明，注释内容和AST都可以增强UniXcoder。


## 摘要
编程语言（programming language）的预训练模型最近在代码智能（code intelligence）方面取得了巨大成功。为了**同时支持与代码相关的理解和生成任务**，最近的工作试图预训练统一的 encoder-decoder 模型。然而，这种框架对于自回归任务来说是次优的，特别对代码补全（code completion）这种因推理效率而需要 decoder-only 架构的任务来说。

本文介绍了UniXcoder，这是一种统一的跨模态（Cross-Modal）编程语言预训练模型。
1. 该模型利用带有**前缀适配器**（prefix adapters）的掩码注意力矩阵来控制模型的行为，以同时支持理解和生成任务。
2. 利用AST和代码注释（code comment）等跨模式内容来增强代码表示。
	- 为了并行编码表示为树的AST，我们提出了一种一对一映射方法，将AST转换为保留树中所有结构信息的序列结构。
	- 此外，我们建议利用多模态内容通过对比学习来学习代码片段的表示，然后使用跨模态生成任务在编程语言之间对齐表示。

我们在9个数据集上对UniXcoder进行了5个代码相关任务的评估。为了进一步评估代码片段表示的性能，我们还为一个新任务构建了一个数据集，称为零样本代码到代码搜索。结果表明，我们的模型在大多数任务上都达到了最先进的性能，分析表明，评论和AST都可以增强UniXcoder。


## Intro
总之，本文的贡献是：
- 提出了一个统一的跨模态预训练模型，该模型利用多模态内容，即代码注释和AST，来支持与代码相关的理解、生成任务和自回归任务。
- 提出了一种一对一的映射函数，该函数将AST转换为保留AST所有信息的序列，并且可以与源代码和注释并行编码。
- 进一步提出利用代码注释来学习代码片段表示，并构建一个新的数据集用于零样本代码搜索，以评估代码片段表示的质量。

实验结果表明，UniXcoder在大多数下游任务上都有显著改进。


## 相关工作
随着自然语言（NL）处理预训练的巨大成功（Devlin等人，2018；Lewis等人，2019；Raffel等人，2019，Brown等人，2020），人们提出了编程语言的预训练模型来促进代码智能的发展。这些预训练模型通常可分为三类：仅编码器模型、仅解码器模型和编码器-解码器模型

- 仅编码模型Encode-only models：（Kanade等人，2019；Buratti等人，2020；Feng等人，2020年；Guo等人，2020；Wang等人，2022）预训练双向变换，其中每个令牌可以等待彼此。Kanade等人（2019）通过屏蔽语言建模和下一个句子预测目标对Python源代码语料库进行预训练Cubert。Codebert（Feng等人，2020）在六种编程语言的NLPL对等体上进行了一项新的预培训任务，即名称替换令牌检测。Graphcodebert（Guo等人，2020）利用数据流增强代码表示，而Syncobert（Wang等人，2022）通过AST边缘预测和对比学习嵌入抽象语法树。然而，**仅编码器模型需要一个额外的解码器来执行生成任务**，其中该解码器从头开始初始化，并且无法从预培训中获益。
- 从预训练开始。对于仅解码器预训练模型，Svyatkovskiy等人（2020）和Lu等人（2021）分别提出了GPT-C和CodeGPT，这两个模型都是使用单向语言建模进行预训练的，只允许令牌参加前一个令牌，并允许其自身预测下一个令牌。纯解码器模型擅长代码完成等自回归任务，但单向框架**对于理解任务来说是次优**的。
- 最近的一些工作探索了编码器-解码器模型，以支持理解和生成任务。PLBART（Ahmad等人，2021）基于BART（Lewis等人，2019）架构，并使用去噪目标在NL和PL语料库上进行预训练。CodeT5（Wang等人，2021）采用了T5（Raffel等人，2019）模型，该模型考虑了来自标识符的关键令牌类型信息，并允许对下游任务进行多任务学习。TreeBERT（Jiang等人，2021）遵循编码器-解码器-转换器框架，但通过对AST路径建模来利用树结构信息

与当前的统一模型不同，UniXcoder基于多层Transformer，并利用带有前缀适配器的掩码注意力矩阵控制模型的行为，以支持理解和生成任务。与编码器-解码器架构相比，UniXcoder可以更好地应用于自回归任务，如IDE中广泛使用的代码完成，因为该任务在实践中只需要解码器的方式来执行高效的推理。Liu等人（2020）也使用多任务学习对类似的CugLM模型进行了预训练，但他们只关注代码完成，而不是各种任务。此外，我们通过一对一映射函数整合AST的语法信息，该函数将AST转换为序列以增强代码表示。与之前使用AST的预训练模型不同，映射函数保留了AST的所有结构信息，不需要额外的预训练任务（如边缘预测）来隐式学习AST结构。


## 3 Unixcoder
UniXcoder是一个统一的跨模态预训练模型，它利用多模态数据（即代码注释和AST）来预训练代码表示。该模型基于Transformer，并利用带有前缀适配器的掩码注意力矩阵（Dong等人，2019）来控制模型的行为。在下文中，我们首先介绍如何**将多模态数据统一为UniXcoder的输入**（第3.1节），然后介绍模型架构（第3.2节）和预训练任务（第3.3节）。

### 3.1 Input Representation

我们在图1中给出了一个带有注释和AST的python代码示例。可以看出，注释“返回数据的样本算术平均值”高度描述了源代码的功能，提供了有关源代码的关键语义信息。此外，AST提供了丰富的语法信息，例如子树 parameters→ “（data）”表示函数定义中术语（数据）的类型（即参数）。它们都可以用作额外的知识，以增强预训练模型中的代码表示。然而，AST通常表示为树，不能直接用作Transformer的输入。为了将AST与代码注释并行编码，我们提出了算法1中描述的一一映射函数F，将AST转换为保留所有结构信息的序列。
<p align="middle">
  <img src="images/Unixcoder-tree.png" width=60%><br>
</p>
<p align="middle">
  <img src="images/Unixcoder_AST.png" width=60%><br>
</p>
给定一个源代码C，我们取它的注释W={w0，w1，…，wm-1}和扁平的AST令牌序列F（T（C））={c0，c1，…，ck−1}作为输入，其中T（C）是代码AST的根。对于输入格式，我们用前缀将它们连接起来作为输入序列，如图2底部所示，其中**前缀表示模型的工作模式**，下面将对此进行讨论。
## 3.2 模型架构
![[images/Unixcoder_架构.png]]
图2显示了UniXcoder的模型架构。该模型在带前缀 Prefix 的代码注释 Comment 和平铺的 AST 上应用N个变换层，以产生隐状态 $H^N=[{h_0^N，h_1^N，…，h_{n-1}^{N}}]$，其中前缀 $p \in \{[Enc]，[Dec]，[E2D]\}$ 表示模型的行为，例如 $[E2D]$ 表示 UniXcoder 作为编码器-解码器模型工作。每个 transformer 层都包含一个架构上相同的变压器，该变压器使用原始Transformer的注意力机制（Vaswani等人，2017），然后在前一层的输出上使用前馈层。对于第l个变压器层，多头自我注意的输出是通过以下公式计算的（增加Mask矩阵）。
<p align="middle">
  <img src="images/unixcoder-att.png" width=60%><br>
</p>
对于仅编码器模式，我们在输入前添加一个特殊的标记\[Enc]作为前缀，并将掩码矩阵的所有元素设置为0，以允许所有标记相互关联。对于仅解码器模式，使用前缀\[Dec]，并将掩码的上三角部分设置为−∞，以表示每个令牌只能处理自己和之前的令牌。对于编码器-解码器模式，允许源输入中的令牌相互关注，而目标输入中的标记仅关注自身以及源和目标输入中之前的令牌。我们使用\[E2D]前缀来表示UniXcoder作为编码器-解码器模型工作。在预训练阶段，模型参数以不同的模式共享，并针对多个目标进行优化，以支持各种类型的下游任务。

## 3.3 预训练任务
首先使用三个任务对UniXcoder进行预训练，即三种架构的注意力机制：
- 包括掩码语言建模（Devlin等人，2018）
- 单向语言建模（Radford等人，2018年）
- 去噪目标（Raffel等人，2019年）。

使UniXcoder能够支持各种类型的与代码相关的下游任务。然后，我们建议利用多模态数据通过**跨模态生成的对比学习**来学习代码片段嵌入，如图3所示。



三个简化的交叉熵损失如下：
### 3.3.1 掩码语言建模MLM
Masked Language Modeling。对于仅编码器模式，我们遵循Devlin等人（2018）的方法应用掩码语言建模（MLM）预训练任务。特别地，我们从输入序列中采样15%的token作为待处理目标，记为 $S_m$。对这 15%进一步分配：用\[MASK] token 替换其中的80%，用随机token替换 10%，并保持另外10%不变。任务是根据掩码令牌的双向上下文令牌预测掩码令牌的原始令牌，如图2（a）所示。特别是，该模型可以利用来自注释的语义信息和来自AST的语法信息来推断掩码代码令牌，这鼓励模型从不同的知识资源中学习代码表示。目标按方程式3计算，其中 $X^{mask}$ 是掩码输入序列。即，对每个掩码输入序列，累加所有15%的token的交叉熵损失。。
$$loss_{MLM} = -\sum_{x_i \in S_m}\log p(x_i|X^{mask})$$
### 3.3.2 单向语言建模ULM
Unidirectional Language Modeling。我们使用单向语言建模（ULM）预训练任务来预训练仅解码器模式，以支持代码完成等自回归任务，如图2（b）所示。任务根据先前的令牌及其自身 $\{x_0，x_1，..，x_{i−1}\}$ 逐个预测下一个令牌 $x_i$，这可以使用注意力掩码的三角形矩阵来完成：逐渐变长前缀，累加所有next token的交叉熵损失。
$$loss_{ULM} = -\sum_{i=0}^{n-1}\log p(x_i|x_{t<i})$$
### 3.3.3 降噪目标DNS
Denoising Objective ，GLM的技术。DeNoiSing (DNS)预训练目标已被证明对BART（Lewis等人，2019）和T5（Raffel等人，2019年）等编解码器模型非常有效。该任务随机屏蔽任意长度的跨度span，然后在编码器-解码器模式下生成这些被屏蔽的spans。为了更好地支持生成任务，如代码摘要，我们在编码器-解码器模式下使用与T5类似的去噪目标，如图2（c）所示。特别地，我们首先将输入序列拆分为 $\max[\frac{n\times r}{l}, 1]$ 个chunk，然后为每个块随机屏蔽 1 到 $2l-1$ 个令牌的跨度，其中n是输入的长度，r是损坏率（corruption rate），l是屏蔽spans的平均长度。我们分别将损坏率r设置为15%、平均长度l设置为5。所有掩码spans的连接 $\{y0，y1，…，yn−1\}$ 与第k个跨度前面的特殊标记 $[MASK_k]$ 将用作输出。

例如对输入 $[x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, ...]$，可能破坏了3块区域，表示为 $[x0, [MASK_1], x4, [MASK_2], x9, [MASK_3], x15, ...]$，那么输出是这三块spans的预测内容的拼接 $\{[MASK1], y1, y2, ..., [MASK2], yk, yk+1, ..., [MASK3], ...\}$ ，并用各自的 $[MASK_i]$ 作为开头。损失函数如下，对每个被mask掉的y，累积计算交叉熵损失：
$$loss_{DNS} = -\sum_{i=0}^{n-1}\log p(y_i|X^{mask},y_{t<i})$$
![[unxcoder-对比学习.png]]
### 3.3.4 代码片段表示学习
Code Fragment Representation Learning。除了为不同模式设计的上述三个预训练任务外，我们还建议利用多模态数据来学习代码片段 $C_i$ 的语义嵌入̃ $\bar h_i$ 。如图3所示，首先使用UniXcoder对映射扁平化的AST序列（源输入）进行编码得到每个 token 的隐状态（二维），然后对源输入的隐状态上应用均值池层，以获得语义嵌入  $\bar h_i$ （所有token的embedding的均值作为句子的embedding）。为了学习语义嵌入，我们提出了两个预训练任务。一种是<mark style="background: #FFB86CA6;">多模态对比学习（MCL）</mark>，另一种是<mark style="background: #FFB86CA6;">跨模态生成（CMG）</mark>。

#### (1) 多模态对比学习MCL
对于多模态对比学习，我们遵循Gao等人（2021）的研究，对同一个输入 $x_i$，使用两个不同的 hidden dropout mask（轻微扰动） 得到两个高度相似但不完全一致的隐状态 $\bar h_i$ 和 $\bar h^+_i$ ，前者为标准，后者为正样本。并使用同一批中的其他完全不同的输入 $x_j$ 的表示作为反例。损失按方程式6计算，其中b是批量大小，τ是温度超参数，cos（·，·）是两个向量之间的余弦相似度。一个batch计算一次Loss，softmax 相似度，交叉熵损失。
$$loss_{MCL}=-\sum_{i=0}^{b-1} \log \frac{e^{\cos( \bar h_i, \bar h^+_i)/\tau}}{\sum_{j=0}^{b-1} e^{\cos( \bar h_j, \bar h^+_j)/\tau}}$$

#### (2) 跨模态生成CMG
对于跨模态生成，我们要求模型生成其注释 $W=\{w0，w1，…，wm-1\}$。注释描述了代码的功能，这不仅可以帮助模型理解代码语义，还可以通过统一的自然语言描述作为支点，使不同编程语言之间的表示对齐。由于评论的生成取决于代码，它将迫使模型将注释中的语义信息融合到代码的隐藏状态中。损失按方程式7计算，其中X是展平的AST令牌序列。用AST及其对应的自然语言注释。输入AST和注释的prefix，逐个生成注释的next token，交叉熵损失。

$$loss_{CMG} = -\sum_{i=0}^{m-1}\log p(w_i|X,w_{t<i})$$
为了避免模型只学到AST到NL的映射，交换两者：为了学习自然语言的语义嵌入，我们以50%的概率随机交换源输入和目标输入。考虑到在下游任务中显式添加AST将引入额外的成本，如解析时间和增加输入长度（标记化后输入长度增加70%），我们通过预训练隐式地从AST学习知识，并在微调阶段只保留AST的叶子（即源代码）。通过在预训练阶段以50%的概率随机丢弃AST的所有非终端符号，可以缓解这种差距。有关预训练数据集和设置的更多详细信息，请参阅附录B。


## 4 实验
我们在九个公共数据集上对UniXcoder进行了五项任务的评估，包括两项理解任务（§4.2）、两项生成任务（§于4.3）和一项自回归任务（§着4.4）。为了进一步评估代码片段嵌入的性能，我们还提出了一个新任务，称为零样本代码到代码搜索（§4.5）。有关数据集和微调的更多详细信息，请参阅附录C。

### 4.1 baseline
我们将UniXcoder与最先进的预训练模型进行了比较，包括仅编码器、仅解码器和编码器-解码器模型。

对于仅编码器模型，我们考虑使用MLM在文本语料库上预训练Roberta（Liu等人，2019），使用MLM和替换标记检测在NL-PL对上预训练CodeBERT（Feng等人，2020），利用数据流增强代码表示的GraphCodeBERT。

对于仅解码器模型，我们考虑GPT-2（Radford等人，2019）和CodeGPT（Lu等人，2021），其中前者在文本语料库上进行预训练，后者在CodeSearchNet数据集上进行预培训。两者都以ULM为目标

对于编解码器模型，我们主要比较了当前的统一模型PLBART以及CodeT5（Wang等人，2021）。PLBART基于BART，经过470M Python和210M Java函数的预训练，以及使用去噪目标从StackOverflow发布的47M NL帖子。CodeT5改编自T5，考虑了来自标识符的关键令牌类型信息，并允许对下游任务进行多任务学习。

### 4.2 任务类理解
**克隆检测任务**：是测量两个代码片段之间的相似性。我们在POJ-104（Mou等人，2016）和BigCloneBench（Svajlenko等人，2014）数据集上进行了实验。第一个数据集是预测两个代码是否具有相同的语义，并使用F1分数作为评估指标，而第二个数据集的目的是在给定代码作为查询的情况下，以平均精度（MAP）作为指标检索语义相似的代码。

**代码搜索**：该任务旨在从给定自然语言查询的候选代码集合中找到最相关的代码。我们在三个数据集上进行了实验，即CSN（Guo等人，2020）、AdvTest（Lu等人，2021）和CosQA（Huang等人，2021年）。CSN数据集由六种编程语言的CodeSearchNet数据集构建，低质量查询由手工规则过滤。AdvTest规范化python函数和变量名，以更好地测试模型的理解和泛化能力。CosQA的代码库也来自CodeSearchNet语料库，但查询来自微软Bing搜索引擎的搜索日志。我们使用平均倒数排名（MRR）评估指标的任务。

结果：结果如表1所示。与仅编码器预训练模型（即第一组）和编码器-解码器模型（即第二组）相比，UniXcoder的表现优于它们，并在所有五个数据集的两个任务上实现了最先进的性能。通过与最后六行消融研究的结果进行比较，我们可以看到，改进主要来自对比学习和多模态的使用。

### 4.3 生成类任务
代码摘要：该任务旨在生成代码段的NL摘要。我们使用CodeXGLUE团队（Lu等人，2021）提供的数据集来完成这项任务。我们使用平滑的BLEU-4（Lin和Och，2004）作为评估指标，并报告六个PL的总分，包括Ruby、JavaScript、Go、Python、Java和PHP。

代码生成任务：是根据NL描述生成代码段。我们使用CONCODE（Iyer等人，2018）数据集，其中输入由NL描述和代码环境组成。对于这项任务，我们使用精确匹配（EM）和BLEU-4作为评估指标。

结果：从表2中可以看出，UniXcoder在生成任务上的性能与CodeT5基础相当，代码生成精度提高了0.3%。然而，UniXcoder在代码摘要和生成任务上的BLEU-4得分略差。主要原因可能来自两个方面。一个是预训练数据中NL-PL对的数量。如表中的消融研究（见无注释）所示，NL-PL对在两项任务上都有显著改善。Wang等人（2021）从Github收集了50%以上的NL-PL对，用于预训练CodeT5。由于收集的数据不是公开的，我们不能使用它来预训练UniXcoder以进行公平比较。另一个原因是模型尺寸。CodeT5基础使用12层编码器和12层解码器，比其他基线和UniXcoder大两倍。因此，我们还列出了使用6层编码器和6层解码器的CodeT5 small的结果。我们可以看到UniXcoder的表现优于CodeT5 small。

### 4.4 代码补全
我们在CodeXGLUE（Lu等人，2021）中使用PY150（Raychev等人，2016）和Github Java语料库（Allamanis和Sutton，2013）数据集进行行级代码完成任务。该任务需要完成一整行代码，并使用精确匹配精度和Levenshtein编辑相似性进行评估（Svyatkovskiy等人，2020）。

在实践中，该任务需要仅使用解码器的方式来执行有效的推理。因此，我们首先将UniXcoder与表3中的仅解码器模型（第一组）进行比较。正如我们所看到的，UniXcoder实现了相当的性能这两个数据集在java语料库上的准确率提高了2.3%，这证明了我们的代码完成模型的有效性。此外，我们还与当前的统一模型（第二组）进行了比较。由于它们基于encoderdecoder框架，我们通过在编码器中输入占位符来微调它们的解码器。结果表明，UniXcoder的性能优于PLBART和CodeT5，这表明我们的模型框架更好地应用于代码完成任务。

### 4.5 零样本代码到代码搜索：新提出
为了进一步评估代码片段嵌入的性能，我们还提出了一个称为零样本代码到代码搜索的新任务。**给定一个源代码作为查询，该任务旨在从零样本设置中的候选集合中检索具有相同语义的代码**。该任务可以通过检索具有相同语义的源代码来帮助用户从一个PL翻译到另一个PL。我们从Ruby/Python/Java PL的CodeNet语料库（Puri等人，2021）中收集了11744/15594/23530个函数。每个函数解决了4053个问题中的一个。我们将每个函数视为一个查询，并从每个PL中检索解决同一问题的所有函数。我们使用平均MAP分数作为评估指标。有关数据集和示例的更多详细信息，请参阅附录C.6。

我们使用最后隐藏状态的均值向量或CLS向量重新实现了公开发布的预训练模型，并将结果报告在表4中。第一行是查询PL，第二行是目标PL。从表中可以看出，UniXcoder实现了最先进的性能，与GraphCodeBERT相比，总分提高了约11分。消融研究进一步表明，多模态数据和代码片段表示预训练任务都可以增强UniXcoder。

### 4.6 模型分析
表征预训练的效果：我们进行消融研究，通过删除对比学习任务（w/o constras）和跨模态生成任务（w/o-cross-gen）来分析代码片段表征预训练任务的效果。如表1和表4所示，两个预训练任务显著提高了理解任务。以零样本代码搜索任务为例，去除对比学习后，性能从20.45%下降到13.73%。此外，如表2和表3所示，这两项预训练任务也使生成任务略有改进。总体而言，消融研究证明了两项预训练任务的有效性。

我们还研究了多模态数据的影响。通过删除注释（w/o注释），表中的结果表明，代码注释在理解和生成任务中都起着重要作用。对于AST（不含AST），我们观察到注入AST可以提高所有代码理解任务的性能。然而，AST并没有对生成任务带来改进，这可能需要一种更好的方法来将AST整合到生成任务中。总的来说，AST和注释都可以改进UniXcoder。

遍历算法的比较我们将我们的映射函数与用于将树映射到序列的其他映射函数进行了比较，即BFS和DFS算法。正如我们所看到的，在用BFS或DFS算法替换我们的映射函数后，UniXcoder在理解和生成任务上的性能都下降了，这证明了我们映射函数的有效性。特别是，使用BFS或DFS算法甚至会损害UniXcoder在某些任务上的性能，因为它将不使用BFS（DFS）与不含AST。主要原因可能是BFS和DFS算法不是一对一的映射函数，可能会将树与另一种结构混淆。

案例研究：我们还进行了一个案例研究，直观地展示了UniXcoder的有效性，如图4所示。我们给出了一个在CosQA数据集上执行代码搜索任务并从不同模型输出预测的示例。查询“python dict rank by value”来自微软Bing搜索引擎的搜索日志。我们知道，用户的意图是在Python语言中按字典的值对其进行排序。尽管PLBART的预测具有更高的词汇重叠，如“秩”和“值”，但该函数是不正确的，因为基础真值的输入应该是字典。我们可以看到UniXcoder检索到一个正确的函数，其输入是一个字典。此外，尽管查询中的“值”在函数定义中表示为语句“key=lambda t:t[1]”，但UniXcoder可以理解代码语义并成功检索到基础真值，这证明了UniXcoders的有效性。

## 5 总结
为了支持与代码相关的理解和生成任务，我们提出了UniXcoder，这是一个统一的预训练模型，整合了来自代码注释和AST的语义和语法信息。我们提出了一种一对一的映射方法，将AST转换为序列结构，并提出了两个新的预训练任务来学习代码片段表示。为了进一步研究代码表示的性能，我们提出了一种新的下游要求零样本代码到代码搜索，并为此任务创建数据集。实验表明，UniXcoder在大多数任务上的表现明显优于之前的工作。进一步的消融研究还表明，AST和代码注释都可以增强UniXcoder，并揭示我们提出的映射函数和预训练任务的有效性。

