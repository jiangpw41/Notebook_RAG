ACL 2022, 中山大学和Microsoft。

题目：UniXcoder: Unified Cross-Modal Pre-training for Code Representation（代码表示的统一跨模态预训练）。
论文：https://arxiv.org/abs/2203.03850

Unified：代码理解和生成使用一个模型。
Cross-Modal跨模态：指代码文本、AST、注释等不同形式的内容。

训练目标：三种类型的语言建模任务进行预训练
- MLM掩码语言建模（Devlin等人，2018）
- 单向语言建模unidirectional language modeling（Radford等人，2018年）
- 去噪目标denoising objective（Raffel等人，2019年）

两个预训练任务来学习可以表示代码片段语义的embedding
- 多模态对比学习，它利用AST来增强代码片段嵌入的语义
- 跨模态生成，它利用代码注释来对齐编程语言之间的embedding。

核心创新
- 带有前缀适配器的掩码注意力矩阵mask attention matrices：控制模型的行为，同一套参数通过“<mark style="background: #FF5582A6;">前缀+掩码</mark>”实现三种模式：Encoder / Decoder / Encoder-Decoder。
	- 解决痛点：架构割裂。编码器-解码器模型在自回归任务（如代码补全）上效率低；纯编码器/解码器又无法兼顾代码理解+生成
- 使用<mark style="background: #FF5582A6;">多模态</mark>训练内容：
	- 解决痛点：信息单一。以往工作仅把代码当纯文本，或使用 AST/数据流但任务单一
	- 提出一种一一映射方法将树形AST映射为序列并保留结构信息（可唯一性映射回树形结构）。
	- 提出使用多模态内容（注释、AST）进行对比学习 contrastive learning，学习代码片段的表示；然后使用跨模式生成任务在编程语言之间对齐表示。

在9个数据集上对UniXcoder进行了5个代码相关任务的评估。为了进一步评估代码片段表示的性能，我们还为一个新任务构建了一个数据集，称为<mark style="background: #FF5582A6;">零样本代码到代码搜索</mark> zero-shot code-to-code search。结果表明，我们的模型在大多数任务上都达到了最先进的性能，分析表明，注释内容和AST都可以增强UniXcoder。


## 摘要
编程语言（programming language）的预训练模型最近在代码智能（code intelligence）方面取得了巨大成功。为了**同时支持与代码相关的理解和生成任务**，最近的工作试图预训练统一的 encoder-decoder 模型。然而，这种框架对于自回归任务来说是次优的，特别对代码补全（code completion）这种因推理效率而需要 decoder-only 架构的任务来说。

本文介绍了UniXcoder，这是一种统一的跨模态（Cross-Modal）编程语言预训练模型。
1. 该模型利用带有**前缀适配器**（prefix adapters）的掩码注意力矩阵来控制模型的行为，以同时支持理解和生成任务。
2. 利用AST和代码注释（code comment）等跨模式内容来增强代码表示。
	- 为了并行编码表示为树的AST，我们提出了一种一对一映射方法，将AST转换为保留树中所有结构信息的序列结构。
	- 此外，我们建议利用多模态内容通过对比学习来学习代码片段的表示，然后使用跨模态生成任务在编程语言之间对齐表示。

我们在9个数据集上对UniXcoder进行了5个代码相关任务的评估。为了进一步评估代码片段表示的性能，我们还为一个新任务构建了一个数据集，称为零样本代码到代码搜索。结果表明，我们的模型在大多数任务上都达到了最先进的性能，分析表明，评论和AST都可以增强UniXcoder。


## Intro
总之，本文的贡献是：
- 提出了一个统一的跨模态预训练模型，该模型利用多模态内容，即代码注释和AST，来支持与代码相关的理解、生成任务和自回归任务。
- 提出了一种一对一的映射函数，该函数将AST转换为保留AST所有信息的序列，并且可以与源代码和注释并行编码。
- 进一步提出利用代码注释来学习代码片段表示，并构建一个新的数据集用于零样本代码搜索，以评估代码片段表示的质量。

实验结果表明，UniXcoder在大多数下游任务上都有显著改进。


## 相关工作
随着自然语言（NL）处理预训练的巨大成功（Devlin等人，2018；Lewis等人，2019；Raffel等人，2019，Brown等人，2020），人们提出了编程语言的预训练模型来促进代码智能的发展。这些预训练模型通常可分为三类：仅编码器模型、仅解码器模型和编码器-解码器模型

- 仅编码模型Encode-only models：（Kanade等人，2019；Buratti等人，2020；Feng等人，2020年；Guo等人，2020；Wang等人，2022）预训练双向变换，其中每个令牌可以等待彼此。Kanade等人（2019）通过屏蔽语言建模和下一个句子预测目标对Python源代码语料库进行预训练Cubert。Codebert（Feng等人，2020）在六种编程语言的NLPL对等体上进行了一项新的预培训任务，即名称替换令牌检测。Graphcodebert（Guo等人，2020）利用数据流增强代码表示，而Syncobert（Wang等人，2022）通过AST边缘预测和对比学习嵌入抽象语法树。然而，**仅编码器模型需要一个额外的解码器来执行生成任务**，其中该解码器从头开始初始化，并且无法从预培训中获益。
- 从预训练开始。对于仅解码器预训练模型，Svyatkovskiy等人（2020）和Lu等人（2021）分别提出了GPT-C和CodeGPT，这两个模型都是使用单向语言建模进行预训练的，只允许令牌参加前一个令牌，并允许其自身预测下一个令牌。纯解码器模型擅长代码完成等自回归任务，但单向框架**对于理解任务来说是次优**的。
- 最近的一些工作探索了编码器-解码器模型，以支持理解和生成任务。PLBART（Ahmad等人，2021）基于BART（Lewis等人，2019）架构，并使用去噪目标在NL和PL语料库上进行预训练。CodeT5（Wang等人，2021）采用了T5（Raffel等人，2019）模型，该模型考虑了来自标识符的关键令牌类型信息，并允许对下游任务进行多任务学习。TreeBERT（Jiang等人，2021）遵循编码器-解码器-转换器框架，但通过对AST路径建模来利用树结构信息

与当前的统一模型不同，UniXcoder基于多层Transformer，并利用带有前缀适配器的掩码注意力矩阵控制模型的行为，以支持理解和生成任务。与编码器-解码器架构相比，UniXcoder可以更好地应用于自回归任务，如IDE中广泛使用的代码完成，因为该任务在实践中只需要解码器的方式来执行高效的推理。Liu等人（2020）也使用多任务学习对类似的CugLM模型进行了预训练，但他们只关注代码完成，而不是各种任务。此外，我们通过一对一映射函数整合AST的语法信息，该函数将AST转换为序列以增强代码表示。与之前使用AST的预训练模型不同，映射函数保留了AST的所有结构信息，不需要额外的预训练任务（如边缘预测）来隐式学习AST结构。


## 3 Unixcoder
UniXcoder是一个统一的跨模态预训练模型，它利用多模态数据（即代码注释和AST）来预训练代码表示。该模型基于Transformer，并利用带有前缀适配器的掩码注意力矩阵（Dong等人，2019）来控制模型的行为。在下文中，我们首先介绍如何**将多模态数据统一为UniXcoder的输入**（第3.1节），然后介绍模型架构（第3.2节）和预训练任务（第3.3节）。

### 3.1 Input Representation
我们在图1中给出了一个带有注释和AST的python代码示例。可以看出，注释“返回数据的样本算术平均值”高度描述了源代码的功能，提供了有关源代码的关键语义信息。此外，AST提供了丰富的语法信息，例如子树参数→ “（数据）”表示函数定义中术语（数据）的类型（即参数）。它们都可以用作额外的知识，以增强预训练模型中的代码表示。然而，AST通常表示为树，不能直接用作Transformer的输入。为了将AST与代码注释并行编码，我们提出了算法1中描述的一一映射函数F，将AST转换为保留所有结构信息的序列。![[Unixcoder-tree.png]]
![[Unixcoder_AST.png]]