
 [Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)（用1B训练对训练有史以来最好的句子嵌入模型） 项目。
句子嵌入模型的质量可以通过以下方式轻松提高：
- 更大、更多样化的训练数据
- 更大的批量

![[embedding_batch_number.png]]


## Pooling
**Embedding模型**（如BERT、Sentence-BERT等）通常需要将**变长的输入序列**（如句子或段落）转换为**固定长度的向量表示**。由于这些模型的原始输出是每个token的向量（序列长度×向量维度），而下游任务（如分类、检索）通常需要单个向量，因此需要一种 **池化（Pooling）** 操作来聚合这些token向量。

以下是几种典型的池化策略及其意义：
1. **Mean Pooling（均值池化）**
	- **做法**：对所有token的向量取**平均值**。
	- **公式**： embedding=N1​i=1∑N​tokeni​
	- **特点**：
	    - 简单高效，适用于大多数场景。
	    - 对所有token平等对待，可能受无关词（如停用词）干扰。
	- **用途**：Sentence-BERT的默认池化方式。
2. **Max Pooling（最大池化）**
	- **做法**：对每个特征维度取所有token中的**最大值**。
	- **特点**：
	    - 保留最显著的特征，可能更适合关键词敏感的任务。
	    - 忽略词序和上下文关系。
3. **[CLS] Token Pooling**
	- **做法**：直接使用模型输出的特殊标记`[CLS]`的向量作为整体表示。
	- **特点**：
	    - BERT预训练时`[CLS]`被设计为聚合全局信息。
	    - 微调时效果较好，但未经微调时可能表现一般。
4.  **Weighted Pooling（加权池化）**
	- **做法**：根据token的重要性（如TF-IDF、注意力权重）加权求和。
	- **例子**： embedding=i=1∑N​wi​⋅tokeni​
	- **特点**：更灵活，但需额外计算权重。
5. **Dynamic Pooling（动态池化）**
	- **做法**：结合多种池化方式（如均值+最大值），或通过小型神经网络学习池化策略。
	- **用途**：高级模型（如SimCSE）可能采用此类方法